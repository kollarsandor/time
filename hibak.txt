ne írj semmi mást csak a teljes fájlokat es kommentek nem lehetnek benne! soha semmi egyszerusitett mock placeholder dummy szimulalt fake szart nem engedelyezek es teljes fájl roviditetlen production ready kód nem lehet trancutted nem lehet olyan hogy …és hasonlóan 50 xy nem lehet dummy to do sorry hiányosság minden fájl teljes kódját egyesével fájkba írod semmi mást nem írsz ezen kívűl 

Feladat: a jelenlegi repo (minden fájl a gyökérben: ./) PRODUCTION READY állapotba hozása kompromisszum nélkül.
Korlátok: a modell és minden modell-változat marad ugyanaz (GLM-4.7-FP8), nincs “könnyítés”, nincs minőségromlás, nincs throughput kompromisszum. Cél: 3000 token/s (hardware-függő, de a szoftver ne legyen szűk keresztmetszet), és elsőre deployolható legyen.

Kiindulás: a jelenlegi kód a repo rootban (./). Ne feltételezz semmilyen “bi/” vagy más mappa-struktúrát, azt te hozd létre rendezett formában a rootból kiindulva. Ne dobd el a native CUDA utat – erre építs. A jelenlegi HTTPServer prototípust cseréld le production-grade ASGI stackre, és a scheduler/engine integráció legyen valódi continuous batching több párhuzamos kérés között.

Kötelező deliverable-ok (mindet implementáld):

1) Production ASGI szerver (FastAPI + uvicorn):
   - Új modulok/fájlok a rootból szervezetten:
     - javasolt: ./src/bi_server/ (vagy ./src/server/), de a pontos név mindegy, csak legyen tiszta és csomagolt
   - Endpointok:
     - GET /healthz (liveness)
     - GET /readyz (readiness: model loaded + worker loop fut)
     - GET /metrics (Prometheus)
     - POST /v1/completions (OpenAI-kompatibilis, stream és non-stream)
     - POST /v1/chat/completions (OpenAI-kompatibilis, stream és non-stream)
   - Streaming: SSE (text/event-stream) delta-k tokenenként.
   - Request validáció (pydantic), max body size, max prompt tokens, max_new_tokens hard cap, timeout policy.
   - Konstans és dokumentált response schema, korrekt HTTP status kódok.

2) Engine architektúra átalakítás (rootból):
   - Legyen dedikált EngineWorker loop (thread vagy asyncio task), ami folyamatosan hívja az engine.step()-et és kezeli a queue-t.
   - A request handler nem “tekeri” a decode-ot. Csak enqueue → await Future/stream.
   - Eredmény visszaadás:
     - Non-stream: várja a teljes befejezést.
     - Stream: tokenenként küldje a deltas-t.
   - Backpressure: max_concurrent_requests + queue limit. Telítésnél 429/503.

3) Native API bővítés (ha kell a streaminghez):
   - Ha a jelenlegi C API csak final outputot ad: egészítsd ki úgy, hogy lehessen rész-tokeneket lekérdezni (pl. get_partial_result) vagy callback/ring-buffer megoldással.
   - Legyen thread-safe és jól dokumentált a Python wrapperben (ctypes argtypes/restype rendben).
   - Ne rontsd a teljesítményt: a streaming path ne legyen kötelező overhead non-stream esetben.

4) Konfig és modell betöltés:
   - A --model-dir alatt olvasd be a config/metát (ha van), és validáld szigorúan (max_seq_len, vocab, rope, fp8 layout).
   - Hibáknál early-fail, érthető error message.
   - Egységes CLI flag-ek a root CLI entrypointon:
     --model-dir, --host, --port, --max-concurrency, --max-batch-tokens, --max-seq-len, --log-level, stb.

5) Observability:
   - Structured logging (JSON): request_id, prompt_tokens, gen_tokens, ttft_ms, tps, batch_size átlag, kv_pages.
   - Prometheus metrikák: request_count, error_count, latency hist, ttft hist, tokens_generated, tps gauge, queue_depth, gpu_mem (ha elérhető).
   - /readyz csak akkor OK, ha worker loop fut és engine init sikeres volt.

6) Release/packaging a repo rootban:
   - Adj ./pyproject.toml-t, és csomagold a Python részt (src layout).
   - CLI entrypoint: `bi` (vagy `bi-server`), ami tudja: `serve` és `benchmark` (ha van).
   - Adj ./Dockerfile-t (multi-stage build):
     - build stage futtatja a meglévő build scriptet (pl. ./scripts/build.sh ha létezik) és lefordítja a native részt
     - runtime stage minimal, csak runtime függőségek + compiled artifacts
   - Adj ./README.md-t:
     - quickstart (build + run serve + curl OpenAI kompat)
     - tuning tippek (batching, concurrency)
     - streaming példa curl-lel

7) Tesztek:
   - Unit: API schema, request limit, backpressure, worker loop queue, graceful shutdown.
   - Mock/Fake engine implementáció a tesztekhez (ne kelljen GPU a CI-hez).
   - Legalább 1 integration smoke (ha GPU van): serve + 1 completion.
   - `python -m pytest` fusson CPU-only környezetben.

Minőségi elvárások:
- Nincs TODO/FIXME/placeholder.
- Nincs busy-wait: használj condition/event alapú jelzést.
- Graceful shutdown SIGTERM-re (stop accept → drain → exit).
- Konzisztens kódstílus, típusannotációk a publikus felületen.
- Dokumentáld az összes új flag-et és endpointot.

Kimenet:
- Repo-szintű, teljes fájlkészlet változtatás (új fájlokkal együtt).
- Rövid “how to run” és “how to benchmark” leírás.